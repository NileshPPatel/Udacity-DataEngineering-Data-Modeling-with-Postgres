# Project: Data Modeling with Postgres

A startup called Sparkify wants to analyze the data they've been collecting on songs and user activity on their new music streaming app. 
The analytics team is particularly interested in understanding what songs users are listening to. 
Currently, they don't have an easy way to query their data, which resides in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

## Introduction

In order to assist in the analysis required, a database needs to be created, as well as an ETL pipeline to import the data. A Postgres database was created in a star schema with a single fact table joining multiple related dimension tables to optimize queries on song play analysis. An ETL pipeline that transfers data from files in two local directories into tables in Postgres was created using Python and SQL.

## Project Files

The project workspace includes six files:

1. create_tables.py - drops and creates the tables. You run this file to reset the tables before each 
   time the ETL scripts are run.
2. test.ipynb - displays the first few rows of each table to let you check the database.
3. etl.ipynb  - reads and processes a single file from song_data and log_data and loads the data into the
   tables. This notebook contains detailed instructions on the ETL process for each of the tables.
4. etl.py  - reads and processes files from song_data and log_data and loads them into the tables.  
5. sql_queries.py - contains all the sql queries.
6. README.md - This file provides discussion on the project.

## Data Sets

There are 2 datasets that need to be imported into the database to be used for anaylsis:

1. Song Dataset
   The first dataset is a subset of real data from the Million Song Dataset. 
   Each file is in JSON format and contains metadata about a song and the artist of that song. 
2. Log Dataset
   The second dataset consists of log files in JSON format generated by the event simulator based on the 
   songs in the dataset above. These simulate activity logs from a music streaming app based on specified 
   configurations.

## Usage

These are the instructions to load the data into the database:

1. Run the create_tables.py file in a Python Terminal to create your database and tables. This will drop 
   any existing tables and create new tables in the database.

```bash
python3 create_tables.py
```

2. Run etl.py in a Python Terminal to process the entire datasets. 

```bash
python3 etl.py
```

3. Run test.ipynb to confirm the creation of your tables with the correct columns and to confirm the
   records were successfully inserted into each table.
